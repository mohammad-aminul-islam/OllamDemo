# Running AI Model Locally on Your PC

To run an AI model locally on your computer, follow these steps:

1. **Install Ollama Runtime**
   First, you need to install the Ollama runtime from the [Ollama official site](https://ollama.com/download).

2. **Install Your AI Model**
   Then, you can install your desired AI model from this list available on the [Ollama website](https://ollama.com/search).

3. **Running the Installed Model**
   If you want to run the `mistral` model, for example, execute the following command in the command prompt:
   ```
   ollama run mistral
   ```

4. **Chat with the Installed Model**
   After successful installation, you can chat with it directly from the command prompt.

## Connecting AI Model to .NET Application

1. **Install Required Nuget Packages**
   To connect the AI model with your .NET application, you need to install the following nuget packages:
   ```
   Microsoft.Extensions.AI
   Microsoft.AspNetCore.OpenApi
   OllamaSharp
   ```
2. **Explore the ApplicationChatClient for AI Model connection**

   **This mark down is generated by my local AI model**
