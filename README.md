<<<<<<< HEAD
# Running AI Model Locally on Your PC
<<<<<<< HEAD

To run an AI model locally on your computer, follow these steps:

1. **Install Ollama Runtime**
   First, you need to install the Ollama runtime from the [Ollama official site](https://ollama.com/download).

2. **Install Your AI Model**
   Then, you can install your desired AI model from this list available on the [Ollama website](https://ollama.com/search).

3. **Running the Installed Model**
   If you want to run the `mistral` model, for example, execute the following command in the command prompt:
   ```
   ollama run mistral
   ```

4. **Chat with the Installed Model**
   After successful installation, you can chat with it directly from the command prompt.

## Connecting AI Model to .NET Application

1. **Install Required Nuget Packages**
   To connect the AI model with your .NET application, you need to install the following nuget packages:
   ```
   Microsoft.Extensions.AI
   Microsoft.AspNetCore.OpenApi
   OllamaSharp
   ```
2. **Explore the ApplicationChatClient for AI Model connection**

   **This mark down is generated by my local AI model**
=======
=======
>>>>>>> d1e98a3 (Create README.md)
To run AI Model in you local pc, follow these steps-
1. First install ollama runtime from the Ollama official site. [https://ollama.com/download]
2. Then you need to install your AI model from this list.[https://ollama.com/search]
3. Like, If you want to install mistral model run the following command in the command prompt <br>
   ```ollama run mistral```
4. After installing successfully, you can chat with it from the command prompt.

   Connect with .NET API
1. To connect the AI Model with your .net application, you need to install this nuget package <br>
     ```Microsoft.Extensions.AI```
    <br>
    ```Microsoft.AspNetCore.OpenApi```<br>
    ```OllamaSharp```<br>
<<<<<<< HEAD
>>>>>>> d1e98a3 (Create README.md)
=======

To run an AI model locally on your computer, follow these steps:

1. **Install Ollama Runtime**
   First, you need to install the Ollama runtime from the [Ollama official site](https://ollama.com/download).

2. **Install Your AI Model**
   Then, you can install your desired AI model from this list available on the [Ollama website](https://ollama.com/search).

3. **Running the Installed Model**
   If you want to run the `mistral` model, for example, execute the following command in the command prompt:
   ```
   ollama run mistral
   ```

4. **Chat with the Installed Model**
   After successful installation, you can chat with it directly from the command prompt.

## Connecting AI Model to .NET Application

1. **Install Required Nuget Packages**
   To connect the AI model with your .NET application, you need to install the following nuget packages:
   ```
   Microsoft.Extensions.AI
   Microsoft.AspNetCore.OpenApi
   OllamaSharp
   ```
2. **Explore the ApplicationChatClient for AI Model connection**

   **This mark down is generated by my local AI model**
>>>>>>> f5660e2 (Revise README for AI model setup and .NET integration)
=======
>>>>>>> d1e98a3 (Create README.md)
